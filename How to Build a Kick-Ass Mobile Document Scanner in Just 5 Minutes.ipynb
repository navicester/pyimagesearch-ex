{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulation-period",
   "metadata": {},
   "source": [
    "# How to Build a Kick-Ass Mobile Document <font color='blue'>Scanner</font> in Just 5 Minutes\n",
    "\n",
    "Building a document scanner with OpenCV can be accomplished in just three simple steps:\n",
    "\n",
    "**Step 1**: Detect edges.\n",
    "\n",
    "**Step 2**: Use the edges in the image to find the contour (outline) representing the piece of paper being scanned.\n",
    "\n",
    "**Step 3**: Apply a perspective transform to obtain the top-down view of the document.\n",
    "\n",
    "Really. That’s it.\n",
    "\n",
    "Only three steps and you’re on your way to submitting your own document scanning app to the App Store.\n",
    "\n",
    "Sound interesting?\n",
    "\n",
    "Read on. And unlock the secrets to build a mobile scanner app of your own.\n",
    "\n",
    "OpenCV and Python versions:\n",
    "\n",
    "This example will run on Python 2.7/3+ and OpenCV 2.4/3+\n",
    "\n",
    "https://youtu.be/yRer1GC2298\n",
    "\n",
    "<video src=\"videocourse/Building a Kick-Ass Document Scanner using Computer Vision, OpenCV, and Python.mp4\" width=\"900px\" height=\"580px\" controls=\"controls\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-metabolism",
   "metadata": {},
   "source": [
    "Last week I gave you a special treat — my very own ```transform.py```  module that I use in all my computer vision and image processing projects. [You can read more about this module here](https://pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/).\n",
    "\n",
    "Whenever you need to perform a 4 point perspective transform, you should be using this module.\n",
    "\n",
    "And you guessed it, we’ll be using it to build our very own document scanner.\n",
    "\n",
    "So let’s get down to business.\n",
    "\n",
    "Open up your favorite Python IDE, (I like Sublime Text 2), create a new file, name it scan.py , and let’s get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rough-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "BASE_DIR = os.path.abspath(os.path.dirname('__file__'))\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "# import the necessary packages\n",
    "from pyimagesearch.transform import four_point_transform\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-implementation",
   "metadata": {},
   "source": [
    "Lines 6-11 handle importing the necessary Python packages that we’ll need.\n",
    "\n",
    "We’ll start by importing our ```four_point_transform```  function which I discussed last week.\n",
    "\n",
    "We’ll also be using the ```imutils```  module, which contains convenience functions for resizing, rotating, and cropping images. You can read more about imutils  in my this [post](https://pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/). To install imutils , simply:\n",
    "\n",
    "> $ pip install --upgrade imutils\n",
    "\n",
    "Next up, let’s import the ```threshold_local```  function from scikit-image. This function will help us obtain the “black and white” feel to our scanned image.\n",
    "\n",
    "Note (15 January 2018): The ```threshold_adaptive```  function has been deprecated. This post has been updated to make use of ```threshold_local``` .\n",
    "\n",
    "Lastly, we’ll use ```NumPy``` for numerical processing, ```argparse```  for parsing command line arguments, and ```cv2```  for our OpenCV bindings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "psychological-identity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# construct the argument parser and parse the arguments\\nap = argparse.ArgumentParser()\\nap.add_argument(\"-i\", \"--image\", required = True,\\n\\thelp = \"Path to the image to be scanned\")\\nargs = vars(ap.parse_args())\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required = True,\n",
    "\thelp = \"Path to the image to be scanned\")\n",
    "args = vars(ap.parse_args())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-property",
   "metadata": {},
   "source": [
    "It handles parsing our command line arguments. We’ll need only a single switch image, ```--image``` , which is the path to the image that contains the document we want to scan.\n",
    "\n",
    "Now that we have the path to our image, we can move on to Step 1: Edge Detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "constant-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "args= {\"image\":\"images/receipt.jpg\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-referral",
   "metadata": {},
   "source": [
    "# Step 1: Edge Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "turned-above",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n"
     ]
    }
   ],
   "source": [
    "# load the image and compute the ratio of the old height\n",
    "# to the new height, clone it, and resize it\n",
    "image = cv2.imread(args[\"image\"])\n",
    "ratio = image.shape[0] / 500.0\n",
    "orig = image.copy()\n",
    "image = imutils.resize(image, height = 500)\n",
    "# convert the image to grayscale, blur it, and find edges\n",
    "# in the image\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "edged = cv2.Canny(gray, 75, 200)\n",
    "# show the original image and the edge detected image\n",
    "print(\"STEP 1: Edge Detection\")\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.imshow(\"Edged\", edged)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-intent",
   "metadata": {},
   "source": [
    "First, we load our image off disk on Line 17.\n",
    "\n",
    "In order to speedup image processing, as well as make our edge detection step more accurate, we resize our scanned image to have a height of 500 pixels on Lines 17-20.\n",
    "\n",
    "We also take special care to keep track of the ratio  of the original height of the image to the new height (Line 18) — this will allow us to perform the scan on the original image rather than the resized image.\n",
    "\n",
    "From there, we convert the image from RGB to grayscale on Line 24, perform Gaussian blurring to remove high frequency noise (aiding in contour detection in Step 2), and perform Canny edge detection on Line 26.\n",
    "\n",
    "The output of Step 1 is then shown on Lines 30 and 31.\n",
    "\n",
    "![](images/receipt-edge-detected.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-appreciation",
   "metadata": {},
   "source": [
    "# Step 2: Finding Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lyric-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Find contours of paper\n"
     ]
    }
   ],
   "source": [
    "# find the contours in the edged image, keeping only the\n",
    "# largest ones, and initialize the screen contour\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "# loop over the contours\n",
    "screenCnt = None\n",
    "for c in cnts:\n",
    "\t# approximate the contour\n",
    "\tperi = cv2.arcLength(c, True)\n",
    "\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\t# if our approximated contour has four points, then we\n",
    "\t# can assume that we have found our screen\n",
    "\tif len(approx) == 4:\n",
    "\t\tscreenCnt = approx\n",
    "\t\tbreak\n",
    "# show the contour (outline) of the piece of paper\n",
    "print(\"STEP 2: Find contours of paper\")\n",
    "#print(screenCnt,type(screenCnt))\n",
    "if screenCnt.any():\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "cv2.imshow(\"Outline\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ec512",
   "metadata": {},
   "source": [
    "```if screenCnt:```报错\n",
    "\n",
    "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-hunter",
   "metadata": {},
   "source": [
    "# Step 3: Apply a Perspective Transform & Threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "constant-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Apply perspective transform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the four point transform to obtain a top-down\n",
    "# view of the original image\n",
    "warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "# convert the warped image to grayscale, then threshold it\n",
    "# to give it that 'black and white' paper effect\n",
    "warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "T = threshold_local(warped, 11, offset = 10, method = \"gaussian\")\n",
    "warped = (warped > T).astype(\"uint8\") * 255\n",
    "# show the original and scanned images\n",
    "print(\"STEP 3: Apply perspective transform\")\n",
    "cv2.imshow(\"Original\", imutils.resize(orig, height = 650))\n",
    "cv2.imshow(\"Scanned\", imutils.resize(warped, height = 650))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-observer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
